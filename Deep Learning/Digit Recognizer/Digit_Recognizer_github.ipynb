{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Digit_Recognizer_github.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"11wASnkKgqySO7ciP5QiYNkj1M_4sl6yZ","authorship_tag":"ABX9TyPWz4Cq8LBJaSUN51gMTcWH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# File : Digit_Recognizer_github\n","# Name : Yu-Ju Fang\n"],"metadata":{"id":"M_uqrpEo6MYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kk0iMHtLceDd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647487889943,"user_tz":240,"elapsed":2535,"user":{"displayName":"方宥儒","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12486434273553538272"}},"outputId":"cfd47923-7ea3-4fea-b002-480cb092ff49"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/digit_recognizer\n"]}],"source":["\n","# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Please enter the address of the file\n","FOLDERNAME = 'Colab\\ Notebooks/digit_recognizer'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/MyDrive/{}'.format(FOLDERNAME))\n","\n","\n","# if it doesn't already exist.\n","%cd drive/MyDrive/$FOLDERNAME\n","# !bash get_datasets.sh\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","#pytorch utility imports\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.utils import make_grid\n"],"metadata":{"id":"DMcqtdg-dxrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#import external libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import os\n","import math\n","%matplotlib inline"],"metadata":{"id":"dekYU86teYCq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["USE_GPU = True\n","\n","dtype = torch.float32 # we will be using float throughout this tutorial\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","# Constant to control how frequently we print train loss\n","print_every = 100\n","\n","print('using device:', device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tCABjJHZG2Y7","executionInfo":{"status":"ok","timestamp":1647487899320,"user_tz":240,"elapsed":14,"user":{"displayName":"方宥儒","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12486434273553538272"}},"outputId":"ba1ffe15-98f5-4e80-d4d1-383d60bd24f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["using device: cpu\n"]}]},{"cell_type":"markdown","source":["### There will be five steps to train an digit recognizer\n","1. Load datasets\n","2. Define Convolutional Neural Network\n","3. Define Loss function and optimizer\n","4. Train the network on the training data\n","5. Test the network on the test data\n","\n","\n","\n"],"metadata":{"id":"fZ3wBr5jtl4l"}},{"cell_type":"markdown","source":["# 1.Load datasets"],"metadata":{"id":"Uzu8LQUbvTiO"}},{"cell_type":"code","source":["# Load train.csv, test.csv, sample_submission.csv\n","train = pd.read_csv('train.csv', dtype=np.float32)\n","final_test = pd.read_csv('test.csv', dtype=np.float32)\n","sample_sub = pd.read_csv(\"sample_submission.csv\")\n","# train.label.head()"],"metadata":{"id":"tnMODGYjd3ic"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Seperate the features and labels\n","targets_np = train.label.values\n","features_np = train.loc[:, train.columns != 'label'].values/255\n","# final_test_np = final_test.values/255\n","\n","\n","# Split into training and test set\n","features_train, features_test, target_train, target_test = train_test_split(features_np, targets_np, test_size=0.2, random_state=42)"],"metadata":{"id":"IJzMwFGSwgFr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features_train = features_train.reshape(features_train.shape[0], 1, 28, 28)\n","print(features_train[0].squeeze().shape)\n","features_test = features_test.reshape(features_test.shape[0], 1, 28, 28)\n","# feature_final_test = final_test_np.reshape(final_test_np.shape[0], 1, 28, 28)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KiLQPp7Y5PvK","executionInfo":{"status":"ok","timestamp":1647487913640,"user_tz":240,"elapsed":17,"user":{"displayName":"方宥儒","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12486434273553538272"}},"outputId":"3e85f7e9-8cfa-4ddd-fc71-f661094ee856"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(28, 28)\n"]}]},{"cell_type":"code","source":["# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n","\n","featuresTrain = torch.from_numpy(features_train)\n","targetsTrain = torch.from_numpy(target_train).type(torch.LongTensor) # data type is long\n","print(featuresTrain.shape)\n","\n","# create feature and targets tensor for test set.\n","featuresTest = torch.from_numpy(features_test)\n","targetsTest = torch.from_numpy(target_test).type(torch.LongTensor) # data type is long\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pzgd5e_XxvkR","executionInfo":{"status":"ok","timestamp":1647487913641,"user_tz":240,"elapsed":15,"user":{"displayName":"方宥儒","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12486434273553538272"}},"outputId":"a89e5af4-db64-4ebe-96f5-6b20a4d8a3ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([33600, 1, 28, 28])\n"]}]},{"cell_type":"code","source":["# Set batch size\n","batch_size = 64\n","\n","# Pytorch train and test sets\n","train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n","test = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n","\n","# data loader\n","train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = True)\n","test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = True)"],"metadata":{"id":"IXWMqMXsx5ZG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.Define Convolutional Neural Network"],"metadata":{"id":"DFk_n_9Lwyij"}},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(1, 6, 3, 1, padding=1)\n","        self.batch1 = nn.BatchNorm2d(6)\n","        self.conv2 = nn.Conv2d(6, 16, 3, 1, padding = 1)\n","        self.batch2 = nn.BatchNorm2d(16)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv3 = nn.Conv2d(16, 64, 3, 1, padding = 1)\n","        self.batch3 = nn.BatchNorm2d(64)\n","        self.conv4 = nn.Conv2d(64, 64, 3, 1, padding = 1)\n","        self.batch4 = nn.BatchNorm2d(64)\n","\n","        self.fc1 = nn.Linear(64*7*7, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","        # self.conv5 = nn.Conv2d(64, 128, 3, 1, padding = 1)\n","        # self.conv6 = nn.Conv2d(128, 128, 3, 1, padding = 1)\n","        # self.conv7 = nn.Conv2d(128, 256, 3, 1, padding = 1)\n","       \n","\n","    def forward(self, x):\n","        # N * 1 * 28 * 28\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.batch1(x)\n","\n","        # N * 6 * 14 * 14\n","        x = F.relu(self.conv2(x))\n","        x = self.batch2(x)\n","\n","        # N * 16 * 14 * 14  \n","        x = F.relu(self.conv3(x))\n","        x = self.batch3(x)\n","        \n","        # N * 64 * 14 * 14  \n","        x = F.relu(self.conv4(x))\n","        x = self.batch4(x)    \n","        x = self.pool(F.relu(self.conv4(x)))\n","\n","        # N * 64 * 7 * 7 \n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        \n","        return x\n","      \n"],"metadata":{"id":"EEljizKDzQI9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = Net()"],"metadata":{"id":"wR9BFD0V05kP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Define Loss function and optimizer"],"metadata":{"id":"g6T3rJMww-EP"}},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.0002)"],"metadata":{"id":"dT_bT_uI1Izf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 10\n","print_every = 50\n","train_losses, test_losses = [], []\n"],"metadata":{"id":"8_ZvcCAM2pfc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Train the network on the training data"],"metadata":{"id":"oGW89M0ixJJo"}},{"cell_type":"code","source":["for epoch in range(epochs):  # loop over the dataset multiple times\n","    net.train()\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        \n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        \n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % print_every == 0:    # print every 50 mini-batches\n","            test_loss = 0\n","            accuracy = 0\n","\n","            # Turn off gradients for validation and test the accuracy of the current model with test dataset\n","            with torch.no_grad():\n","                net.eval()\n","                for images, labels in test_loader:\n","                    log_ps = net(images)\n","                    test_loss += criterion(log_ps, labels)\n","\n","                    ps = torch.exp(log_ps)\n","                    # Get our top predictions\n","                    top_p, top_class = ps.topk(1, dim=1)\n","                    equals = top_class == labels.view(*top_class.shape)\n","                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n","\n","            net.train()\n","\n","            train_losses.append(running_loss/len(train_loader))\n","            test_losses.append(test_loss/len(test_loader))\n","\n","            print(\"Epoch: {}/{}   \".format(epoch+1, epochs),\n","                  \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n","                  \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n","                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n","\n","print('Finished Training')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mexneDTs1JGJ","outputId":"428f035c-edb1-4a8f-b2f8-e11cb875f16e","executionInfo":{"status":"ok","timestamp":1647488378815,"user_tz":240,"elapsed":344152,"user":{"displayName":"方宥儒","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12486434273553538272"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/2    Training Loss: 0.000..  Test Loss: 0.128..  Test Accuracy: 0.960\n","Epoch: 1/2    Training Loss: 0.012..  Test Loss: 0.120..  Test Accuracy: 0.964\n","Epoch: 1/2    Training Loss: 0.023..  Test Loss: 0.102..  Test Accuracy: 0.969\n","Epoch: 1/2    Training Loss: 0.032..  Test Loss: 0.094..  Test Accuracy: 0.972\n","Epoch: 1/2    Training Loss: 0.040..  Test Loss: 0.088..  Test Accuracy: 0.973\n","Epoch: 1/2    Training Loss: 0.048..  Test Loss: 0.085..  Test Accuracy: 0.974\n","Epoch: 1/2    Training Loss: 0.056..  Test Loss: 0.072..  Test Accuracy: 0.979\n","Epoch: 1/2    Training Loss: 0.062..  Test Loss: 0.078..  Test Accuracy: 0.976\n","Epoch: 1/2    Training Loss: 0.068..  Test Loss: 0.072..  Test Accuracy: 0.979\n","Epoch: 1/2    Training Loss: 0.074..  Test Loss: 0.068..  Test Accuracy: 0.978\n","Epoch: 1/2    Training Loss: 0.079..  Test Loss: 0.070..  Test Accuracy: 0.978\n","Epoch: 2/2    Training Loss: 0.000..  Test Loss: 0.060..  Test Accuracy: 0.982\n","Epoch: 2/2    Training Loss: 0.006..  Test Loss: 0.064..  Test Accuracy: 0.980\n","Epoch: 2/2    Training Loss: 0.012..  Test Loss: 0.066..  Test Accuracy: 0.979\n","Epoch: 2/2    Training Loss: 0.016..  Test Loss: 0.055..  Test Accuracy: 0.984\n","Epoch: 2/2    Training Loss: 0.021..  Test Loss: 0.057..  Test Accuracy: 0.983\n","Epoch: 2/2    Training Loss: 0.025..  Test Loss: 0.056..  Test Accuracy: 0.982\n","Epoch: 2/2    Training Loss: 0.029..  Test Loss: 0.053..  Test Accuracy: 0.983\n","Epoch: 2/2    Training Loss: 0.034..  Test Loss: 0.052..  Test Accuracy: 0.983\n","Epoch: 2/2    Training Loss: 0.038..  Test Loss: 0.053..  Test Accuracy: 0.983\n","Epoch: 2/2    Training Loss: 0.042..  Test Loss: 0.051..  Test Accuracy: 0.984\n","Epoch: 2/2    Training Loss: 0.046..  Test Loss: 0.057..  Test Accuracy: 0.982\n","Finished Training\n"]}]},{"cell_type":"code","source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","plt.plot(train_losses, label='Training loss')\n","plt.plot(test_losses, label='Validation loss')\n","plt.legend(frameon=False)"],"metadata":{"id":"7Z-vpEqS6umg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Test the network on the test data"],"metadata":{"id":"1Sop1iIpxR7y"}},{"cell_type":"code","source":["final_test_np = final_test.values/255\n","feature_final_test = final_test_np.reshape(final_test_np.shape[0], 1, 28, 28)\n","test_tn = torch.from_numpy(feature_final_test)\n"],"metadata":{"id":"IJytGu7dPCzn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating fake labels for convenience of passing into DataLoader\n","fake_labels = np.zeros(final_test_np.shape)\n","fake_labels = torch.from_numpy(fake_labels)"],"metadata":{"id":"PVl0vjacPFLu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission_tn_data = torch.utils.data.TensorDataset(test_tn, fake_labels)\n","submission_loader = torch.utils.data.DataLoader(submission_tn_data, batch_size = batch_size, shuffle = False)\n"],"metadata":{"id":"Hr-fY2UvPGg0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Making it submission ready\n","submission = [['ImageId', 'Label']]\n","\n","# Turn off gradients for validation\n","with torch.no_grad():\n","    net.eval()\n","    image_id = 1\n","    for images, _ in submission_loader:\n","        log_ps = net(images)\n","        ps = torch.exp(log_ps)\n","        top_p, top_class = ps.topk(1, dim=1)\n","        \n","        for prediction in top_class:\n","            submission.append([image_id, prediction.item()])\n","            image_id += 1"],"metadata":{"id":"WyhevKnvPMmK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Outfile the result of prediction into csv file"],"metadata":{"id":"8IHwgXYO21aY"}},{"cell_type":"code","source":["submission_df = pd.DataFrame(submission)\n","submission_df.columns = submission_df.iloc[0]\n","submission_df = submission_df.drop(0, axis=0)"],"metadata":{"id":"dVjCTP79PT1n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission_df.to_csv(\"submission.csv\", index=False)"],"metadata":{"id":"WZ0GdyVxPVyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"J4_VvqScPXY2"},"execution_count":null,"outputs":[]}]}